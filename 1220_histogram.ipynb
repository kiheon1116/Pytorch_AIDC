{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42345/1496075520.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch, gc\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict, Iterable, Callable\n",
    "from bn_fold import fuse_bn_recursively\n",
    "\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "from scipy.stats import norm \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input : tensor(numpy array)[-1,64] --> output : flag(numpy array)[]\n",
    "# (block(16word * 64) * n개) => [-1,64], 1개당 int16\n",
    "def SR(x, length):  \n",
    "     # sign reduction : 1개의 data(2B) 기준 상위 9bit 비교  \n",
    "     x_numpy = x.view(np.int16)\n",
    "     sr_result = x_numpy\n",
    "     sr_flag = 0\n",
    "     ########################### (16:8) ###########################     \n",
    "     # ### 0000_0000_0111_1111 이하,  1111_1111_1000_0000 이상인지. 해당하면 1을 저장.\n",
    "     is_sr_array         = ((x_numpy < 0x80) & (x_numpy >= 0)) | (x_numpy <= -128)\n",
    "     \n",
    "     # first data(word)에 대한 특수한 조건. (10bit 비교) ### 0000_0000_0011_1111 이하, 1111_1111_1100_0000 이상인지.\n",
    "     first_element_flag  = ((x_numpy[:,0] < 0x40  & (x_numpy[:,0] >=0)) | (x_numpy[:,0] <= -64)).reshape(-1,1)\n",
    "     ############################################################\n",
    "\n",
    "    #  ########################### (16:9) ###########################     \n",
    "    #  # ### 0000_0000_1111_1111 이하,  1111_1111_0000_0000 이상인지. 해당하면 1을 저장.\n",
    "    #  is_sr_array         = ((x_numpy < 256) & (x_numpy >= 0)) | (x_numpy <= -256)\n",
    "     \n",
    "    #  # first data(word)에 대한 특수한 조건. (10bit 비교) ### 0000_0000_0011_1111 이하, 1111_1111_1100_0000 이상인지.\n",
    "    #  first_element_flag  = ((x_numpy[:,0] < 0x80  & (x_numpy[:,0] >=0)) | (x_numpy[:,0] <= -128)).reshape(-1,1)\n",
    "    # #  ############################################################\n",
    "\n",
    "\n",
    "    #  ########################### (16:10) 1.6:1 ###########################     \n",
    "    #  # ### 0000_0000_0111_1111 이하,  1111_1111_1000_0000 이상인지. 해당하면 1을 저장.\n",
    "    #  is_sr_array         = ((x_numpy < 512) & (x_numpy >= 0)) | (x_numpy <= -512)\n",
    "     \n",
    "    #  # first data(word)에 대한 특수한 조건. (10bit 비교) ### 0000_0000_0011_1111 이하, 1111_1111_1100_0000 이상인지.\n",
    "    #  first_element_flag  = ((x_numpy[:,0] < 256  & (x_numpy[:,0] >=0)) | (x_numpy[:,0] <= -256)).reshape(-1,1)\n",
    "    #  ############################################################\n",
    "\n",
    "\n",
    "    #  ########################### (16:11) ##########################     \n",
    "    #  # ### 0000_0000_0111_1111 이하,  1111_1111_1000_0000 이상인지. 해당하면 1을 저장.\n",
    "    #  is_sr_array         = ((x_numpy < 1024) & (x_numpy >= 0)) | (x_numpy <= -1024)\n",
    "     \n",
    "    #  # first data(word)에 대한 특수한 조건. (10bit 비교) ### 0000_0000_0011_1111 이하, 1111_1111_1100_0000 이상인지.\n",
    "    #  first_element_flag  = ((x_numpy[:,0] < 512  & (x_numpy[:,0] >=0)) | (x_numpy[:,0] <= -512)).reshape(-1,1)\n",
    "    #  ############################################################\n",
    "\n",
    "    #  ########################### (16:12) 1.5:1 ###########################     \n",
    "     # ### 0000_0000_0111_1111 이하,  1111_1111_1000_0000 이상인지. 해당하면 1을 저장.\n",
    "    #  is_sr_array         = ((x_numpy < 2048) & (x_numpy >= 0)) | (x_numpy <= -2048)\n",
    "     \n",
    "    #  # first data(word)에 대한 특수한 조건. (10bit 비교) ### 0000_0000_0011_1111 이하, 1111_1111_1100_0000 이상인지.\n",
    "    #  first_element_flag  = ((x_numpy[:,0] < 1024  & (x_numpy[:,0] >=0)) | (x_numpy[:,0] <= -1024)).reshape(-1,1)\n",
    "    #  ############################################################\n",
    "\n",
    "    #  ########################### (16:13) ###########################     \n",
    "    #  # ### 0000_0000_0111_1111 이하,  1111_1111_1000_0000 이상인지. 해당하면 1을 저장.\n",
    "    #  is_sr_array         = ((x_numpy < 4096) & (x_numpy >= 0)) | (x_numpy <= -4096)\n",
    "     \n",
    "    #  # first data(word)에 대한 특수한 조건. (10bit 비교) ### 0000_0000_0011_1111 이하, 1111_1111_1100_0000 이상인지.\n",
    "    #  first_element_flag  = ((x_numpy[:,0] < 2048  & (x_numpy[:,0] >=0)) | (x_numpy[:,0] <= -2048)).reshape(-1,1)\n",
    "    #  ############################################################\n",
    "\n",
    "    #  ########################### (16:14) ###########################     \n",
    "    #  # ### 0000_0000_0111_1111 이하,  1111_1111_1000_0000 이상인지. 해당하면 1을 저장.\n",
    "    #  is_sr_array         = ((x_numpy < 8192) & (x_numpy >= 0)) | (x_numpy <= -8192)\n",
    "     \n",
    "    #  # first data(word)에 대한 특수한 조건. (10bit 비교) ### 0000_0000_0011_1111 이하, 1111_1111_1100_0000 이상인지.\n",
    "    #  first_element_flag  = ((x_numpy[:,0] < 4096  & (x_numpy[:,0] >=0)) | (x_numpy[:,0] <= -4096)).reshape(-1,1)\n",
    "    # #  ############################################################\n",
    "\n",
    "    #  ########################### (16:15) ###########################     \n",
    "    #  # ### 0000_0000_0111_1111 이하,  1111_1111_1000_0000 이상인지. 해당하면 1을 저장.\n",
    "    #  is_sr_array         = ((x_numpy < 16384) & (x_numpy >= 0)) | (x_numpy <= -16384)\n",
    "     \n",
    "    #  # first data(word)에 대한 특수한 조건. (10bit 비교) ### 0000_0000_0011_1111 이하, 1111_1111_1100_0000 이상인지.\n",
    "    #  first_element_flag  = ((x_numpy[:,0] < 8192  & (x_numpy[:,0] >=0)) | (x_numpy[:,0] <= -8192)).reshape(-1,1)\n",
    "    #  ############################################################ \n",
    "\n",
    "     # is_sr_array[-1,64] 일 때, 모든 2차원 (word 단위의 64개)가 sign reduction 가능한지. \n",
    "     row_flag            = (np.sum(is_sr_array, axis=1) == length).reshape(-1,1) # length = 64\n",
    "     sr_flag             = row_flag & first_element_flag\n",
    "     \n",
    "     return sr_flag\n",
    "\n",
    "def ZRLE(x, length): \n",
    "     # 1개의 word(2B) 기준 단위로 zero, non_zero 파악 이후 encoding\n",
    "     x_numpy = x.view(np.int16)\n",
    "    \n",
    "     # word 단위(int16)로 non_zero의 경우 flag = 1인 tensor\n",
    "     # 즉, tensor는 Byte 단위이므로, int16 -> 1B로 표현해서\n",
    "     # 4 word를 묶어 64bit 단위로 pattern을 파악하기 위해서는 4B씩 묶어야 하므로 \n",
    "     # 기존의 1B씩 된 mask를 int32로 표현하여 4개씩 묶음.\n",
    "     non_zero_element         = (x_numpy != 0)\n",
    "     \n",
    "     non_zero_element_pattern = non_zero_element.view(np.int32)\n",
    "     \n",
    "     non_zero_size      = (non_zero_element_pattern == 0x00000000) * 6\n",
    "     \n",
    "     non_zero_size     += (non_zero_element_pattern == 0x01000000) * 22\n",
    "     non_zero_size     += (non_zero_element_pattern == 0x00010000) * 21\n",
    "     non_zero_size     += (non_zero_element_pattern == 0x00000100) * 21\n",
    "     non_zero_size     += (non_zero_element_pattern == 0x00000001) * 21\n",
    "     \n",
    "     non_zero_size     += (non_zero_element_pattern == 0x01010000) * 36\n",
    "     non_zero_size     += (non_zero_element_pattern == 0x01000100) * 36\n",
    "     non_zero_size     += (non_zero_element_pattern == 0x01000001) * 36\n",
    "     non_zero_size     += (non_zero_element_pattern == 0x00010100) * 36\n",
    "     non_zero_size     += (non_zero_element_pattern == 0x00010001) * 36\n",
    "     non_zero_size     += (non_zero_element_pattern == 0x00000101) * 36\n",
    "     \n",
    "     non_zero_size     += (non_zero_element_pattern == 0x01010100) * 52\n",
    "     non_zero_size     += (non_zero_element_pattern == 0x01010001) * 52\n",
    "     non_zero_size     += (non_zero_element_pattern == 0x01000101) * 52\n",
    "     non_zero_size     += (non_zero_element_pattern == 0x00010101) * 52\n",
    "     \n",
    "     non_zero_size     += (non_zero_element_pattern == 0x01010101) * 66\n",
    "\n",
    "     zrle_flag            = (np.sum(non_zero_size,axis=1) < ((length/2)*16)-2).reshape(-1,1) # length 64기준 510  # 510 574 638 702 766 830 894 958\n",
    "\n",
    "     return zrle_flag\n",
    "     \n",
    "def BPC(input, length):\n",
    "     input_np = input.view(np.int16)\n",
    "     row = input_np.shape[0]\n",
    "     \n",
    "     # Data Block\n",
    "     input_np_base = input_np[:,0].reshape(-1,1)\n",
    "     base = input_np_base.copy()\n",
    "     base_uint8 = base.view(np.uint8)\n",
    "     base_unpackbits = np.unpackbits(base_uint8, bitorder = 'little')\n",
    "     BASE = base_unpackbits.reshape((row,16))[:,::-1]\n",
    "     \n",
    "     # delta\n",
    "     input_np_delta = input_np[:,1:].reshape(-1,length-1) - input_np_base # 64기준 length - 1 = 63\n",
    "     \n",
    "     \n",
    "     # Delta-BP\n",
    "     delta_uint8 = input_np_delta.view(np.uint8) # 현재 little endian\n",
    "     delta_unpackbits = np.unpackbits(delta_uint8, bitorder ='little')\n",
    "     delta_unpackbits_2d = delta_unpackbits.reshape((row,length-1,16))[:,:,::-1] # 64기준 length-1 = 63\n",
    "     Delta_BP = np.swapaxes(delta_unpackbits_2d, 1, 2) # (row,16,63)\n",
    "     Delta_BP_CNT = np.sum(Delta_BP, axis=2) # (row,16)\n",
    "     \n",
    "     \n",
    "     # DBP_XOR\n",
    "     DBP_XOR = Delta_BP.copy()\n",
    "     DBP_XOR[:,1:,:] ^= DBP_XOR[:,:15,:] # (row,16,63)\n",
    "     DBP_XOR_CNT = np.sum(DBP_XOR, axis=2) # (row,16)\n",
    "     DBP_XOR_SUM = np.zeros((DBP_XOR.shape[0],DBP_XOR.shape[1],DBP_XOR.shape[2]+1), dtype = np.uint8)\n",
    "     DBP_XOR_SUM[:,:,0] = BASE\n",
    "     DBP_XOR_SUM[:,:,1:] = DBP_XOR\n",
    "     DBP_XOR_packbits =  np.packbits(DBP_XOR_SUM.reshape(-1,))\n",
    "     DBP_XOR_SYMBOL = DBP_XOR_packbits.view(np.uint64).reshape(row,-1,1) ######################################################################## length 변경시 uint(length)로 변경해주어야함.\n",
    "     \n",
    "     \n",
    "     \n",
    "     # Encoding\n",
    "          \n",
    "     ## Case 1 : All-0's \n",
    "     mask_all_zero  = (DBP_XOR_CNT == 0)\n",
    "     mask_all_zero_left  = np.zeros(mask_all_zero.shape, dtype=np.uint8)\n",
    "     mask_all_zero_right = np.zeros(mask_all_zero.shape, dtype=np.uint8)     \n",
    "     mask_all_zero_left[:,:15] = (DBP_XOR_CNT[:,1:] == 0)\n",
    "     mask_all_zero_left[:,1:] = (DBP_XOR_CNT[:,:15] == 0)\n",
    "     case_1 = (DBP_XOR_CNT == 0) & ((mask_all_zero & mask_all_zero_left) == 0) & ((mask_all_zero & mask_all_zero_right) == 0)\n",
    "     ### All-0's run length 2~16\n",
    "     case_0 = (DBP_XOR_CNT == 0) & ~case_1\n",
    "     \n",
    "     ## Case 2 : All-1's \n",
    "     case_2 = (DBP_XOR_CNT == length-1) # 64기준 length - 1 = 63\n",
    "     \n",
    "     ## Case 3 : DBP_XOR != 0 & Delta_BP == 0\n",
    "     dbx_dbp_flag = (DBP_XOR_CNT != 0) & (Delta_BP_CNT ==0)\n",
    "     case_3 = ~case_2 & (dbx_dbp_flag == 1)\n",
    "     \n",
    "     ## Case 4 : Consecutive two 1's \n",
    "     mask_two_consec = DBP_XOR_SYMBOL & (DBP_XOR_SYMBOL-1)\n",
    "     mask_two_consec_result = (DBP_XOR_CNT == 2).reshape(row,16,1) & (DBP_XOR_SYMBOL == (mask_two_consec + (mask_two_consec >> 1))) \n",
    "     mask_two_consec_flag = mask_two_consec_result.reshape(-1,16)\n",
    "     case_4 = ~case_2 & ~case_3 & (mask_two_consec_flag == 1)\n",
    "\n",
    "     ## Case 5 : Single 1\n",
    "     case_5 = ~case_2 & ~case_3 & ~case_4 & (DBP_XOR_CNT == 1)\n",
    "     \n",
    "     ## Case 6 : Uncompressed\n",
    "     case_6 = (~case_0) & (~case_1) & (~case_2) & (~case_3) & (~case_4) & (~case_5)\n",
    "\n",
    "     result_flag = np.zeros((DBP_XOR.shape),dtype=np.int16)\n",
    "     # result_flag = (case_0)*0 + (case_1)*3 + (case_2)*5 + (case_3)*5 + (case_4)*11 + (case_5)*11+ (case_6)*64   \n",
    "     result_flag = (case_0)*0 + (case_1)*3 + (case_2)*5 + (case_3)*6 + (case_4)*11 + (case_5)*12+ (case_6)*64   \n",
    "\n",
    "     mask_all_zero_2 = np.zeros((DBP_XOR.shape[0],DBP_XOR.shape[1]+2), dtype=np.uint8)\n",
    "     mask_all_zero_2_left = np.zeros((DBP_XOR.shape[0],DBP_XOR.shape[1]+2), dtype=np.uint8)\n",
    "     mask_all_zero_2[:,1:17] = case_0\n",
    "     mask_all_zero_2_left[:,:17] = mask_all_zero_2[:,1:]\n",
    "     mask_all_zero_2_sum = np.sum((mask_all_zero_2 ^ mask_all_zero_2_left), axis=1)//2\n",
    "     \n",
    "     result_sum = np.sum(result_flag, axis=1) + mask_all_zero_2_sum*int(6)\n",
    "     result = (result_sum <= (length*16-18)) # 64기준 length*16-18 = 512-18 \n",
    "     # 494,  558, 622, 686, 750, 814, 878, 942\n",
    "     # 8bit -> 2:1(512), 7bit -> 16:9(576), 6bit -> 16:10(640),  5bit -> 16:11(704), \n",
    "     # 4bit -> 16:12(768), 3bit -> 16:13(832), 2bit -> 16:14(896), 1bit -> 16:15(960)\n",
    "     \n",
    "     return result\n",
    "\n",
    "     \n",
    "\n",
    "def Comp(x_tensor : torch.Tensor, length):      # Tensor (128,3,224,224) -> Tensor (64,3,224,224)\n",
    "    x = x_tensor.cpu().numpy().view(np.int16)\n",
    "    x_numpy = x.reshape(-1,length) # 94080 16\n",
    "    sr_flag = SR(x_numpy,length)\n",
    "    # Do ZRLE\n",
    "     \n",
    "    zrle_flag = ZRLE(x_numpy,length)\n",
    "    # Do BPC\n",
    "     \n",
    "    bpc_flag = BPC(x_numpy,length)\n",
    "\n",
    "    # print('sr_flag : ', np.sum(sr_flag.astype(np.int32)) / len(sr_flag.reshape(-1,)))\n",
    "    # print('zrle_flag : ', np.sum(zrle_flag.astype(np.int32)) / len(zrle_flag.reshape(-1,)))\n",
    "    # print('bpc_flag : ', np.sum(bpc_flag.astype(np.int32)) / len(bpc_flag.reshape(-1,)))\n",
    "    return sr_flag | zrle_flag | bpc_flag.reshape(-1,1)\n",
    "\n",
    "def rolling(flag, x_numpy):\n",
    "    lossy_result = x_numpy*(~flag)\n",
    "    print(\"lossy result\",lossy_result.shape, lossy_result.dtype)\n",
    "    rolling_0 = (((lossy_result >= 0x0000) & (lossy_result < 128)) | ((lossy_result <= -128) & (lossy_result > -256)))#.view(np.int16)\n",
    "    rolling_1 = (((lossy_result >= 128) & (lossy_result < 256)) | ((lossy_result <= -256) & (lossy_result > -512)))#.view(np.int16)\n",
    "    rolling_2 = (((lossy_result >= 256) & (lossy_result < 512)) | ((lossy_result <= -512) & (lossy_result > -1024)))#.view(np.int16)\n",
    "    rolling_3 = (((lossy_result >= 512) & (lossy_result < 1024)) | ((lossy_result <= -1024) & (lossy_result > -2048)))#.view(np.int16)\n",
    "    rolling_4 = (((lossy_result >= 1024) & (lossy_result < 2048)) | ((lossy_result <= -2048) & (lossy_result > -4096)))#.view(np.int16)\n",
    "    rolling_5 = (((lossy_result >= 2048) & (lossy_result < 4096)) | ((lossy_result <= -4096) & (lossy_result > -8192)))#.view(np.int16)\n",
    "    rolling_6 = (((lossy_result >= 4096) & (lossy_result < 8192)) | ((lossy_result <= -8192) & (lossy_result > -16384)))#.view(np.int16)\n",
    "    rolling_7 = (((lossy_result >= 8192) & (lossy_result < 16384)) | ((lossy_result <= -16384) & (lossy_result > -32768)))#.view(np.int16)\n",
    "    rolling_8 = (((lossy_result >= 16384) & (lossy_result < 32768)) | ((lossy_result <= -32768) & (lossy_result > -65536)))#.view(np.int16)\n",
    "    rolling_9 = (((lossy_result >= 32768) & (lossy_result < 65536)) | (lossy_result <= -65536) )#.view(np.int16)\n",
    "\n",
    "    INT7_index = rolling_0*0+rolling_1*1+rolling_2*2+rolling_3*3+rolling_4*4+rolling_5*5+rolling_6*6+rolling_7*7+rolling_8*8+rolling_9*9\n",
    "    INT7 = rolling_0&0x007f+rolling_1&0x00fe+rolling_2&0x01fc+rolling_3&0x03f8+rolling_4&0x07f0+rolling_5&0x0fe0+rolling_6&0x1fc0+rolling_7&0x3f80+rolling_8&0x7f00+rolling_9&0xfe00\n",
    "    return INT7.view(np.int16)\n",
    "     \n",
    "def rolling_index(flag, x_torch):\n",
    "    result = x_torch*(~flag)\n",
    "    rolling_0 = (((result >= 0x0000) & (result < 128)) | ((result <= -128) & (result > -256)))#.view(np.int16)\n",
    "    rolling_1 = (((result >= 128) & (result < 256)) | ((result <= -256) & (result > -512)))#.view(np.int16)\n",
    "    rolling_2 = (((result >= 256) & (result < 512)) | ((result <= -512) & (result > -1024)))#.view(np.int16)\n",
    "    rolling_3 = (((result >= 512) & (result < 1024)) | ((result <= -1024) & (result > -2048)))#.view(np.int16)\n",
    "    rolling_4 = (((result >= 1024) & (result < 2048)) | ((result <= -2048) & (result > -4096)))#.view(np.int16)\n",
    "    rolling_5 = (((result >= 2048) & (result < 4096)) | ((result <= -4096) & (result > -8192)))#.view(np.int16)\n",
    "    rolling_6 = (((result >= 4096) & (result < 8192)) | ((result <= -8192) & (result > -16384)))#.view(np.int16)\n",
    "    rolling_7 = (((result >= 8192) & (result < 16384)) | ((result <= -16384) & (result > -32768)))#.view(np.int16)\n",
    "    rolling_8 = (((result >= 16384) & (result < 32768)) | ((result <= -32768) & (result > -65536)))#.view(np.int16)\n",
    "    rolling_9 = (((result >= 32768) & (result < 65536)) | (result <= -65536) )#.view(np.int16)\n",
    "\n",
    "    index = rolling_0*1+rolling_1*2+rolling_2*3+rolling_3*4+rolling_4*5+rolling_5*6+rolling_6*7+rolling_7*8+rolling_8*9+rolling_9*10\n",
    "    return index \n",
    "\n",
    "\n",
    "# mask = [0x80,0xC0,0xE0,0xF0,0xF8,0xFC,0xFE,0xFF]\n",
    "def Decomp(flag,x_numpy,length):\n",
    "     x_numpy_reshape = x_numpy.cpu().numpy().reshape(-1,length).view(np.int16)\n",
    "     result = (x_numpy_reshape & 0xff00) + (x_numpy_reshape & 0x00ff)*flag\n",
    "    #  print(\"x_numpy_reshape\",x_numpy_reshape.shape,x_numpy_reshape.dtype)\n",
    "    #  result_rolling = rolling(flag, x_numpy_reshape)\n",
    "    # #  print(result_rolling.shape, result_rolling.dtype)\n",
    "    #  result = ((x_numpy_reshape & result_rolling)*(~flag)) & ((x_numpy_reshape & 0xffff)*flag)\n",
    "     return torch.tensor(result.astype(np.int16))\n",
    "     \n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "model = fuse_bn_recursively(models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1))#.to('cuda')\n",
    "# model = fuse_bn_recursively(models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1))#.to('cuda'))\n",
    "# model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.IMAGENET1K_V1).to('cuda')\n",
    "#model = fuse_bn_recursively(models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1).to('cuda'))\n",
    "\n",
    "############################################ for FP#######################################\n",
    "# def Quant(x : torch.Tensor, n : int) :\n",
    "\n",
    "#     scale = 0\n",
    "#     zero_p = 0\n",
    "#     N = 2 ** n\n",
    "#     N_MIN, N_MAX = -N//2, N//2 - 1\n",
    "#     x_max, x_min = torch.max(x) , torch.min(x)\n",
    "\n",
    "#     scale = (x_max - x_min) / (N-1)\n",
    "#     scale += (x_max * (scale == 0))\n",
    "#     zero_n = x_max * N_MIN - x_min * N_MAX\n",
    "#     zero_d = x_max - x_min\n",
    "#     zero_p =  torch.round(zero_n / (zero_d + 1e-30)) * (zero_d != 0)\n",
    "\n",
    "#     x_hat = torch.round(x / scale + zero_p)\n",
    "#     x_q   = torch.clip(x_hat, N_MIN, N_MAX).type(torch.int16)\n",
    "\n",
    "#     return x.half(), scale, zero_p\n",
    "     \n",
    "# def DeQuant(    x_q: torch.Tensor, \n",
    "#                 scale: torch.Tensor, \n",
    "#                 zero_p: torch.Tensor):\n",
    "#     # return scale  * (x_q - zero_p)\n",
    "#     return x_q.float()\n",
    "#########################################################################################\n",
    "\n",
    "def Quant(x : torch.Tensor, n : int) :\n",
    "     \n",
    "    N = 2 ** n\n",
    "    N_MIN, N_MAX = -N//2 + 1, N//2 - 1\n",
    "    x_max, x_min = torch.max(x) , torch.min(x)\n",
    "    x_max_abs = torch.abs(x_max)\n",
    "    x_min_abs = torch.abs(x_min)\n",
    "    x_abs_flag = torch.ge(x_max_abs,x_min_abs)\n",
    "    x_max = x_abs_flag*x_max_abs + (~x_abs_flag)*x_min_abs\n",
    "    x_min = -x_max\n",
    "\n",
    "    scale = (x_max - x_min) / (N-2)\n",
    "    scale += (x_max * (scale == 0))\n",
    "    zero_n = x_max * N_MIN - x_min * N_MAX\n",
    "    zero_d = x_max - x_min\n",
    "    zero_p =  torch.round(zero_n / (zero_d + 1e-30)) * (zero_d != 0)\n",
    "\n",
    "    x_hat = torch.round(x / scale + zero_p)\n",
    "    x_q   = torch.clip(x_hat, N_MIN, N_MAX).type(torch.int16)\n",
    "\n",
    "    return x_q, scale, zero_p\n",
    "     \n",
    "def DeQuant(    x_q: torch.Tensor, \n",
    "                scale: torch.Tensor, \n",
    "                zero_p: torch.Tensor):\n",
    "    return scale  * (x_q.to('cuda') - zero_p)\n",
    "\n",
    "def quantize_channel(x: torch.Tensor, n : int):\n",
    "     \n",
    "    N = 2 ** n\n",
    "    N_MIN, N_MAX = -N//2 +1, N//2 - 1\n",
    "\n",
    "    if len(x.shape) >= 4:\n",
    "        x_2d  = x.view(x.shape[0], -1)\n",
    "        x_max = torch.max(x_2d,dim=1)[0]\n",
    "        x_min = torch.min(x_2d,dim=1)[0]\n",
    "        x_max_abs = torch.abs(x_max)\n",
    "        x_min_abs = torch.abs(x_min)\n",
    "        x_abs_flag = torch.ge(x_max_abs,x_min_abs)\n",
    "        \n",
    "        x_max = x_abs_flag*x_max_abs + (~x_abs_flag)*x_min_abs\n",
    "        x_min = -x_max\n",
    "        \n",
    "        scale = ((x_max - x_min) / (N-2))\n",
    "        scale += (x_max * (scale == 0))\n",
    "        scale = scale.view(x.shape[0], -1)\n",
    "\n",
    "        zero_n = x_max * N_MIN - x_min * N_MAX\n",
    "        zero_d = x_max - x_min\n",
    "        zero_p = torch.round(zero_n / (zero_d + 1e-30)) * (zero_d != 0)\n",
    "        zero_p = zero_p.view(x.shape[0], -1)\n",
    "\n",
    "        x_hat = torch.round(x_2d / scale + zero_p)\n",
    "        x_q   = torch.clip(x_hat, N_MIN, N_MAX).view(x.shape).type(torch.int16)\n",
    "        return x_q, scale, zero_p\n",
    "\n",
    "    x_max, x_min = torch.abs(torch.max(x)) , torch.abs(torch.min(x))\n",
    "    x_max_abs = torch.abs(x_max)\n",
    "    x_min_abs = torch.abs(x_min)\n",
    "    x_abs_flag = torch.ge(x_max_abs,x_min_abs)\n",
    "    x_max = x_abs_flag*x_max_abs + (~x_abs_flag)*x_min_abs\n",
    "    x_min = -x_max\n",
    "\n",
    "    scale = (x_max - x_min) / (N-2)\n",
    "    scale += (x_max * (scale == 0))\n",
    "    zero_n = x_max * N_MIN - x_min * N_MAX\n",
    "    zero_d = x_max - x_min\n",
    "    zero_p = torch.round(zero_n / (zero_d + 1e-30)) * (zero_d != 0)\n",
    "\n",
    "    x_hat = torch.round(x / scale + zero_p)\n",
    "    x_q   = torch.clip(x_hat, N_MIN, N_MAX).type(torch.int16)\n",
    "\n",
    "    return x_q, scale, zero_p\n",
    "\n",
    "def dequantize_channel  (   x_q: torch.Tensor, \n",
    "                            scale: torch.Tensor, \n",
    "                            zero_p: torch.Tensor):\n",
    "\n",
    "    x_2d = x_q.view(x_q.shape[0], -1).to('cuda')\n",
    "    return (scale  * (x_2d - zero_p)).view(x_q.shape)\n",
    "\n",
    "# def save_outputs_hook(layer_id) -> Callable:          \n",
    "#     def fn(_, input) :\n",
    "#         with torch.no_grad():\n",
    "#             Quant_input, scale, zero_p = Quant(input[0],16)#quantize_channel(param,16)\n",
    "#             flag = Comp(Quant_input,length)\n",
    "#             Comp_output = Decomp(flag, Quant_input,length).reshape(Quant_input.shape)\n",
    "#         # param[:] = dequantize_channel(Comp_output, scale, zero_p)\n",
    "#             input[0][:] = DeQuant(Comp_output, scale, zero_p)#dequantize_channel(Comp_output, scale, zero_p)\n",
    "     \n",
    "#             # Quant_input, scale, zero_p = Quant(input[0],16)\n",
    "#             # flag = Comp(Quant_input)\n",
    "#             # Comp_output = Decomp(flag,Quant_input).reshape(Quant_input.shape)\n",
    "#             # input[0][:] = DeQuant(Comp_output, scale, zero_p).reshape(input[0].shape)\n",
    "#             # print(layer_id, 'fmap', np.sum(flag) / len(flag.reshape(-1,)))\n",
    "#             # # print(layer_id, input[0][:])\n",
    "#             # input[0][:] = DeQuant(Quant_input, scale, zero_p).reshape(input[0].shape)\n",
    "#             # input[0][:] = DeQuant(torch.tensor(Comp_output), scale, zero_p).reshape(input[0].shape)\n",
    "#             #print(input[0].shape)\n",
    "#     return fn\n",
    "\n",
    "# for name, layer in model.named_modules():\n",
    "#     if (\"layer1\" != name) | (\"layer2\" != name) | (\"layer3\" != name)| (\"layer4\" != name) :\n",
    "#         layer = dict([*model.named_modules()])[name]\n",
    "#         #if isinstance(layer, nn.ReLU):\n",
    "#         # layer.register_forward_pre_hook(save_outputs_hook(name))\n",
    "\n",
    "length = 64\n",
    "# parameters = []\n",
    "parameters = np.zeros([]).astype(np.int16)\n",
    "parameters_index = np.zeros([]).astype(np.int16)\n",
    "\n",
    "for name, param in tqdm(model.named_parameters()):\n",
    "    Data_shape = param.shape\n",
    "    shape_mul = 1\n",
    "    for i in Data_shape:\n",
    "        shape_mul *= i\n",
    "    with torch.no_grad():\n",
    "        if shape_mul%64 != 0 :\n",
    "            Quant_input, scale, zero_p = quantize_channel(param,16)\n",
    "            param[:] = dequantize_channel(Quant_input, scale, zero_p)\n",
    "            # Quant_input, scale, zero_p = Quant(param,16)#quantize_channel(param,16)\n",
    "            # param[:] = DeQuant(Quant_input, scale, zero_p)#dequantize_channel(Comp_output, scale, zero_p)\n",
    "            continue\n",
    "\n",
    "        if 'bn' in name:\n",
    "            Quant_input, scale, zero_p = quantize_channel(param,16)\n",
    "            param[:] = dequantize_channel(Quant_input, scale, zero_p)\n",
    "            # Quant_input, scale, zero_p = Quant(param,16)#quantize_channel(param,16)\n",
    "            # param[:] = DeQuant(Quant_input, scale, zero_p)#dequantize_channel(Comp_output, scale, zero_p)\n",
    "            continue\n",
    "        \n",
    "        if 'bias' in name:\n",
    "            Quant_input, scale, zero_p = quantize_channel(param,16)\n",
    "            param[:] = dequantize_channel(Quant_input, scale, zero_p)\n",
    "            # Quant_input, scale, zero_p = Quant(param,16)#quantize_channel(param,16)\n",
    "            # param[:] = DeQuant(Quant_input, scale, zero_p)#dequantize_channel(Comp_output, scale, zero_p)\n",
    "\n",
    "            continue    \n",
    "        # Quant_input, scale, zero_p = quantize_channel(param,16)                    \n",
    "        Quant_input, scale, zero_p = Quant(param,16)#quantize_channel(param,16)\n",
    "        \n",
    "        # parameters = np.append(parameters.reshape(-1,).tolist(),Quant_input.tolist())\n",
    "        flag = Comp(Quant_input,length)\n",
    "        # Comp_output = Decomp(flag, Quant_input,length).reshape(Quant_input.shape)\n",
    "        param_index = rolling_index(flag,parameters)        \n",
    "        parameters_index = np.append(parameters_index.reshape(-1,).tolist(),param_index.tolist())\n",
    "        \n",
    "        # print(name, param.shape, param.max(), param.min(), np.sum(flag.astype(np.int32)) / len(flag.reshape(-1,)))\n",
    "        \n",
    "        # param[:] = dequantize_channel(Comp_output, scale, zero_p)\n",
    "        param[:] = DeQuant(Quant_input, scale, zero_p)#dequantize_channel(Comp_output, scale, zero_p)\n",
    "\n",
    "\n",
    "from dataprep.eda import plot\n",
    "\n",
    "# value = parameters.reshape(-1)\n",
    "index = parameters.reshape(-1)\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "X = plt.hist(index)\n",
    "plt.show()\n",
    "\n",
    "plot(X).save(\"/home/kkh/pytorch/fig.html\")\n",
    "# with open(\"data.html\",\"w\") as file:\n",
    "#     file.write(index.data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataset = dsets.ImageFolder(\"/media/imagenet/val\", models.ResNet50_Weights.IMAGENET1K_V1.transforms()) ### 2번째 인자, transform\n",
    "# loader = DataLoader(dataset= dataset, # dataset\n",
    "#                    batch_size= 128,   # batch size power to 2\n",
    "#                    shuffle = False, # false\n",
    "#                    num_workers = 8, # num_workers \n",
    "#                    pin_memory=True) # pin_memory \n",
    "\n",
    "# correct = 0\n",
    "# total = 50000\n",
    "# accum = 0\n",
    "# model.eval()\n",
    "# # torch.no_grad()\n",
    "# with torch.no_grad():\n",
    "#     for idx, (input, label) in enumerate(tqdm(loader)):\n",
    "#         input = input.cuda(non_blocking=True)\n",
    "#         label = label.cuda(non_blocking=True)     \n",
    "#         output = model(input)    \n",
    "#         # print(output)\n",
    "#         pred = torch.argmax(output, 1)\n",
    "#         correct += (pred == label).int().sum()\n",
    "#         accum += 4\n",
    "#         # if idx % 1000 == 0:\n",
    "#         #     print(idx, correct /accum * 100, correct, accum)\n",
    "#     acc1 = correct / total * 100\n",
    "\n",
    "# print(acc1)\n",
    "\n",
    "\n",
    "# x = torch.Tensor([[[1,2,4,8,16,32,64,256,512,1024],[0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0]]*62,[[32000,32000,32000,32000,32000,32000,32000,32000,32000,32000]*64]])\n",
    "# y = Comp(x)\n",
    "# print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kkh_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a0bcc4a6c88335402fd18aab6b8baaf35d3cfb2cef319b2b9ecf28f31c9ad5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
