{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data로 부터 직접(directly) tensor 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# data로 부터 직접(directly) tensor 생성하기\n",
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Numpy 배열로부터 생성하기**\n",
    "\n",
    "\n",
    "##### tensor를 Numpy 배열로 변환하기 /  Numpy 배열을  tensor로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n",
      "t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Numpy 배열로부터 생성하기\n",
    "np_array = np.array(data) # data == array\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)\n",
    "\n",
    "# 반대 과정 (Numpy 변환(bridge))\n",
    "# -----------------------------------------------------------\n",
    "# tensor를 Numpy 배열로 변환하기\n",
    "t = torch.ones(5) # tensor\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()    # tensor to numpy : tensor.numpy()\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "\n",
    "t.add_(1) # tensor.add_(x) = 모든 value에 x를 더함.\n",
    "\n",
    "# tensor의 변경 사항이 Numpy 배열에 반영됨.  \n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Numpy 배열을 tensor로 변환하기\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "np.add(n, 1, out=n) # np.add(array, x, out=array) : array의 모든 value에 x를 더해 out으로 array로 출력함.\n",
    "\n",
    "# Numpy 배열의 변경사항이 Tensor에 반영됨.\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones Tensor: \n",
      "tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.8357, 0.8747],\n",
      "        [0.2665, 0.8162]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tensor로 부터 생성하기\n",
    "# 명시적으로 override하지 않는다면, 인자로 주어진 텐서의 속성(모양(shape),자료형(datatype))을 유지함.\n",
    "x_ones =torch.ones_like(x_data)                     # x_data의 속성을 유지함.\n",
    "print(f\"ones Tensor: \\n{x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data의 속성을 덮어씀.\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.8691, 0.2787, 0.7423],\n",
      "        [0.5484, 0.4887, 0.8274]])\n",
      "\n",
      "Ones_Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 무작위(random) 또는 상수(constant) 값을 사용하기\n",
    "shape =(2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor}\\n\")\n",
    "print(f\"Ones_Tensor: \\n {ones_tensor}\\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 5])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# tensor의 속성(Attribute)\n",
    "\n",
    "# tensor의 속성은 텐서의 모양(shape), 자료형(datatype) 및 어느 장치에 저장되는지를 나타냄.\n",
    "\n",
    "tensor = torch.rand(3,5)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\") # default FP32\n",
    "print(f\"Device tensor is stored on: {tensor.device}\") # default = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# tensor operation\n",
    "\n",
    "# 전치 (transposing), 인덱싱(indexing), 슬라이싱(slicing)  등등 100가지 이상의 tensor 연산을 확인 가능.\n",
    "# GPU에서 실행 가능\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "     tensor = tensor.to('cuda')\n",
    "     print(f\"Device tensor is stored on: {tensor.device}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Numpy식의 표준 indexing slicing\n",
    "tensor = torch.ones(4,4)\n",
    "tensor[:,1] = 0 # 2차원에서 index가 1인 모든 값을 0으로 삽입 \n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# tensor 합치기\n",
    "t1 = torch.cat([tensor,tensor,tensor], dim = 1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# tensor 곱하기\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# 다른 문법\n",
    "print(f\"tensor * tensor \\n {tensor*tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 두 tensor 간의 행렬 곱(matrix multiplication)을 계산함.\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)}\\n\")\n",
    "# 다른 문법\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2121., 2030., 2121., 2121.],\n",
      "        [2121., 2030., 2121., 2121.],\n",
      "        [2121., 2030., 2121., 2121.],\n",
      "        [2121., 2030., 2121., 2121.]]) \n",
      "\n",
      "tensor([[2126., 2035., 2126., 2126.],\n",
      "        [2126., 2035., 2126., 2126.],\n",
      "        [2126., 2035., 2126., 2126.],\n",
      "        [2126., 2035., 2126., 2126.]])\n",
      "tensor([[4519876., 4141225., 4519876., 4519876.],\n",
      "        [4519876., 4141225., 4519876., 4519876.],\n",
      "        [4519876., 4141225., 4519876., 4519876.],\n",
      "        [4519876., 4141225., 4519876., 4519876.]])\n"
     ]
    }
   ],
   "source": [
    "# 바꿔치기(in-place) 연산 : 접미사를 갖는 연산들은 바꿔치기(in-place) 연산입니다.\n",
    "# ex) x.copy_()나 x.t_()는 x를 변경함.\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)\n",
    "tensor.copy_(tensor*tensor) # tensor 복사\n",
    "print(tensor)\n",
    "# 바꿔치기 연산은 메모리를 일부 절약하지만, 기록(history)이 즉시 삭제되어 도함수(derivative) 계산에 문제가 발생할 수 있습니다. 따라서 사용 권장 x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('1031_kkh': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f584f2a6c3493bc5ad3f9a7fda70fca86566a62a439156976e7315d3ef22a065"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
